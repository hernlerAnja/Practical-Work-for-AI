{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation & Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env imports\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from Bio.PDB import PDBParser\n",
    "import torchmetrics\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add tankbind directory to System path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tankbind_src_folder_path = \"./tankbind/\"\n",
    "sys.path.insert(0, tankbind_src_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from tankbind\n",
    "from feature_utils import get_protein_feature, get_clean_res_list, extract_torchdrug_feature_from_mol, get_canonical_smiles\n",
    "from utils import construct_data_from_graph_gvp, evaulate_with_affinity, evaulate\n",
    "from model import get_model\n",
    "from generation_utils import get_LAS_distance_constraint_mask, get_info_pred_distance, write_with_new_coords\n",
    "from metrics import print_metrics, myMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f\"dataset_10\"\n",
    "df = pd.read_csv(f'{dataset_path}/Mcule_10000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the protein names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protein_names(file_path):\n",
    "    protein_names = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Get the base name without directory path\n",
    "            base_name = os.path.basename(line.strip())\n",
    "            # Split the base name to get the protein name without extension\n",
    "            protein_name = os.path.splitext(base_name)[0]\n",
    "            protein_names.append(protein_name)\n",
    "    return protein_names\n",
    "\n",
    "file_path = 'pdb_dataset.ds'\n",
    "protein_names = extract_protein_names(file_path)\n",
    "\n",
    "# print(protein_names[0:5]. len(protein_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 10% of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(protein_names) * 0.1\n",
    "sampled_protein_names = random.sample(protein_names, n)\n",
    "\n",
    "# print(sampled_protein_names[0:5], len(sampled_protein_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get protein features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_proteins(protein_names, pdb_directory):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    protein_dict = {}\n",
    "\n",
    "    for proteinName in protein_names:\n",
    "        try:\n",
    "            proteinFile = f\"{pdb_directory}/{proteinName}.pdb\"\n",
    "            s = parser.get_structure(proteinName, proteinFile)\n",
    "            res_list = list(s.get_residues())\n",
    "            clean_res_list = get_clean_res_list(res_list, ensure_ca_exist=True)\n",
    "            protein_dict[proteinName] = get_protein_feature(clean_res_list)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {proteinName}: {e}\")\n",
    "\n",
    "    return protein_dict\n",
    "\n",
    "pdb_directory = \"PDB_files\"  # Directory containing PDB files\n",
    "\n",
    "protein_dict = process_proteins(sampled_protein_names, pdb_directory)\n",
    "\n",
    "# torch.save(protein_dict, 'protein_dict.pt')   # execute only in first notebook run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_names_new = list(protein_dict.keys())  # updated protein names after processing, in case some proteins failed to process\n",
    "\n",
    "# print(len(protein_names_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create protein data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for protein_name in protein_names_new:\n",
    "    for i, line in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = line['smiles']\n",
    "        compund_name = ''\n",
    "        protein_name = protein_name\n",
    "        com = \",\".join([str(x.round(3)) for x in protein_dict[protein_name][0].mean(axis=0).numpy()])\n",
    "        info.append([protein_name, compund_name, smiles, \"protein_center\", com])\n",
    "\n",
    "info = pd.DataFrame(info, columns=['protein_name', 'compound_name', 'smiles', 'pocket_name', 'pocket_com'])\n",
    "\n",
    "# torch.save(info, 'data.pt')   # execute only in first notebook run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset_VS(Dataset):\n",
    "    def __init__(self, root, data=None, protein_dict=None, molecule_dict=None, proteinMode=0, compoundMode=1,\n",
    "                 pocket_radius=20, shake_nodes=None,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data = data\n",
    "        self.protein_dict = protein_dict\n",
    "        self.molecule_dict = molecule_dict\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data = torch.load(self.processed_paths[0])\n",
    "        self.protein_dict = torch.load(self.processed_paths[1])\n",
    "        self.molecule_dict = torch.load(self.processed_paths[2])\n",
    "        self.proteinMode = proteinMode\n",
    "        self.pocket_radius = pocket_radius\n",
    "        self.compoundMode = compoundMode\n",
    "        self.shake_nodes = shake_nodes\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt', 'proteins.pt', 'molecules.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Save data and protein dictionary\n",
    "        torch.save(self.data, self.processed_paths[0])\n",
    "        torch.save(self.protein_dict, self.processed_paths[1])\n",
    "        torch.save(self.molecule_dict, self.processed_paths[2])\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        line = self.data.iloc[idx]\n",
    "        smiles = line['smiles']\n",
    "        pocket_com = line['pocket_com']\n",
    "        pocket_com = np.array(pocket_com.split(\",\")).astype(float) if isinstance(pocket_com, str) else pocket_com\n",
    "        pocket_com = pocket_com.reshape((1, 3))\n",
    "        use_whole_protein = line.get('use_whole_protein', False)\n",
    "\n",
    "        protein_name = line['protein_name']\n",
    "        protein_data = self.protein_dict.get(protein_name)\n",
    "        \n",
    "        if protein_data is None:\n",
    "            raise ValueError(f\"Protein {protein_name} not found in pre-calculated protein dictionary\")\n",
    "\n",
    "        protein_node_xyz, protein_seq, protein_node_s, protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v = protein_data\n",
    "\n",
    "        # Load precomputed molecular features\n",
    "        molecule_data = self.molecule_dict.get(smiles)\n",
    "        if molecule_data is None:\n",
    "            raise ValueError(f\"SMILES {smiles} not found in precomputed molecular dictionary\")\n",
    "        \n",
    "        coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = self.molecule_dict[smiles]\n",
    "\n",
    "        data, input_node_list, keepNode = construct_data_from_graph_gvp(\n",
    "            protein_node_xyz, protein_seq, protein_node_s, protein_node_v, \n",
    "            protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "            coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "            pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "            use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode\n",
    "        )\n",
    "        data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset dirrectory --> only on first run!\n",
    "# os.system(f\"rm -r {dataset_path}\") \n",
    "# os.system(f\"mkdir -p {dataset_path}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precalculate the molecule_dict s.t. the dataset creation is no longer dependent on torchdrug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute only in first notebook run\n",
    "smiles_list = info['smiles'].to_list()\n",
    "smiles_set = set(smiles_list)\n",
    "\n",
    "molecule_dict = {}\n",
    "for molecule in smiles_set:\n",
    "    smiles = get_canonical_smiles(molecule)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    mol.Compute2DCoords()\n",
    "    molecule_dict[molecule] = extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True)\n",
    "\n",
    "torch.save(molecule_dict, 'dataset_10/processed/molecules.pt')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only excute after running the code above once (& only if needed), otherwise FileNotFoundError\n",
    "data = torch.load(f'{dataset_path}/processed/data.pt')\n",
    "protein_dict = torch.load(f'{dataset_path}/processed/proteins.pt')\n",
    "molecule_dict = torch.load(f'{dataset_path}/processed/molecules.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MyDataset_VS(root=dataset_path, data=info, protein_dict=protein_dict, molecule_dict=molecule_dict) # only on first run, otherwise execute line below\n",
    "dataset = MyDataset_VS(root=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 # higher batchsize possible only if enough memmory is available (eg.: 10)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "model = get_model(0, logging, device)\n",
    "\n",
    "# load self-dock model\n",
    "modelFile = \"./model/self_dock.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "_ = model.eval()\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=0) # changed num_workers from 8 to 0, due to multiprocessing error\n",
    "affinity_pred_list = []\n",
    "for data in tqdm(data_loader):\n",
    "    if data.dis_map.shape[0] < 50000: # to filter out proteins with dismap size > 10000 -> 50000 due to batch size 5, can be adjusted as needed (if it's too big, it will cause a memory error)\n",
    "        data = data.to(device)\n",
    "        y_pred, affinity_pred = model(data)\n",
    "        affinity_pred_list.append(affinity_pred.detach().cpu())\n",
    "    else:\n",
    "        affinity_pred_list.append(torch.zeros(5, 1))\n",
    "\n",
    "affinity_pred_list = torch.cat(affinity_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = dataset.data\n",
    "info['affinity'] = affinity_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.to_csv(f\"{dataset_path}/result_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose compound with highest affinity score for each protein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(f'{dataset_path}/result_info.csv') # only execute if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = info.loc[info.groupby('protein_name',sort=False)['affinity'].agg('idxmax')].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = chosen[chosen['affinity'] != 0]\n",
    "chosen_smiles = chosen['smiles'].tolist()\n",
    "\n",
    "chosen_data = MyDataset_VS(root=dataset_path, data=chosen) \n",
    "chosen_data.data = chosen\n",
    "\n",
    "# print(len(chosen_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:03:52   5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\n"
     ]
    }
   ],
   "source": [
    "# get model if previous code not executed in this notebook\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "model = get_model(0, logging, device)\n",
    "\n",
    "# load self-dock model\n",
    "modelFile = \"./model/self_dock.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "_ = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [08:48<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(chosen_data, batch_size=1, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=0)\n",
    "\n",
    "y_preds = []\n",
    "tankbind_list = []\n",
    "old_coords = []\n",
    "new_coords_list = []\n",
    "\n",
    "for i, data_with_batch_info in enumerate(tqdm(data_loader)):\n",
    "\n",
    "    y_pred, affinity_pred = model(data_with_batch_info)\n",
    "\n",
    "    coords = data_with_batch_info.coords.to(device)\n",
    "    old_coords.append(coords)\n",
    "    protein_nodes_xyz = data_with_batch_info.node_xyz.to(device)\n",
    "    n_compound = coords.shape[0] \n",
    "    n_protein = protein_nodes_xyz.shape[0] \n",
    "    y_pred = y_pred.reshape(n_protein, n_compound).to(device).detach()\n",
    "    y_preds.append(y_pred)\n",
    "    y = data_with_batch_info.dis_map.reshape(n_protein, n_compound).to(device)\n",
    "    compound_pair_dis_constraint = torch.cdist(coords, coords)\n",
    "\n",
    "    # Extract SMILES and generate 2D coordinates\n",
    "    smiles = chosen_smiles[i]  \n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    mol.Compute2DCoords()\n",
    "\n",
    "    # Compute LAS distance constraint mask\n",
    "    LAS_distance_constraint_mask = get_LAS_distance_constraint_mask(mol).bool()\n",
    "\n",
    "    # Calculate information\n",
    "    info = get_info_pred_distance(coords, y_pred, protein_nodes_xyz, compound_pair_dis_constraint,\n",
    "                                  LAS_distance_constraint_mask=LAS_distance_constraint_mask,\n",
    "                                  n_repeat=1, show_progress=False)\n",
    "\n",
    "    # Save to file\n",
    "    toFile = f'{dataset_path}/chosen/KIBA_tankbind_{i}.sdf'\n",
    "    new_coords = info.sort_values(\"loss\")['coords'].iloc[0].astype(np.double)\n",
    "    new_coords_list.append(torch.tensor(new_coords))\n",
    "    write_with_new_coords(mol, new_coords, toFile)\n",
    "    \n",
    "    tankbind_list.append([mol, new_coords])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_affinity = np.load(\"KIBA load & process/target_affinity.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.01976171\n"
     ]
    }
   ],
   "source": [
    "chosen_info = chosen_data.data\n",
    "chosen_info['new_affinity'] = target_affinity\n",
    "\n",
    "comp_df = chosen_info['affinity'].compare(chosen_info['new_affinity'])\n",
    "\n",
    "# manual computaion of MAE\n",
    "comp_df['Score_diff'] = (chosen_info['affinity'] - chosen_info['new_affinity']).abs()\n",
    "comp_df_sum = comp_df['Score_diff'].sum() / len(chosen_data)\n",
    "print(\"{:.8f}\".format(comp_df_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSELoss & MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_mae_metric(pred_aff, target_aff):\n",
    "    # Assuming pred_aff and target_aff are already tensors\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        loss = criterion(pred_aff, target_aff)\n",
    "\n",
    "    mae = torchmetrics.functional.mean_absolute_error(pred_aff, target_aff)\n",
    "\n",
    "    return {\"loss\": loss.item(), \"MAE\": mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_aff_tensor= torch.tensor(chosen_info['affinity'].tolist(), requires_grad=True)\n",
    "target_aff_tensor = torch.tensor(chosen_info['new_affinity'].tolist())\n",
    "metrics = mse_mae_metric(chosen_aff_tensor, target_aff_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss:19.993, MAE: 4.020'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(metrics, f'{dataset_path}/MSE_MAE_metrics.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
